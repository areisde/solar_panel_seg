{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d20c17-11fe-4afd-a559-9f8bd156f42d",
   "metadata": {},
   "source": [
    "# Goals and methods\n",
    "In this notebook, we will be using 'Segmentation_models.pytorch' to setup the different neural network architectures and experiment with different parameters. We will be using 'Albumentations' for data augmentation, given we have a pretty small dataset to work with. The whole implementation is based on pytorch, so that will be the 'glue' in our project, to put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd6bea2-a886-4c40-92fa-540fc9971b65",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cdb1160-a3cb-4818-a9e7-29d8a03a38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data tools\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Data visualization\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Data loading\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33177c5c-d859-4c11-8dcc-7d294ebcaa25",
   "metadata": {},
   "source": [
    "# 2. Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d5f41b-3a86-4417-a9db-cf02b7650989",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = 'data/images'\n",
    "MASKS_DIR = 'data/masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66b1f3-3c57-44e2-a8cd-1c8ff921e54a",
   "metadata": {},
   "source": [
    "# 3. Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60740af-0c05-4c1b-be8b-f9664d8ae0d8",
   "metadata": {},
   "source": [
    "### 3.1. Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11eb9c8b-2133-43f3-aefe-e25a1473260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global data directory\n",
    "DATA_DIR = './data/'\n",
    "\n",
    "# Train dataset directory\n",
    "train_file = os.path.join(DATA_DIR, 'train.json')\n",
    "#x_train_dir = os.path.join(DATA_DIR, 'train.json')\n",
    "#y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
    "\n",
    "# Validation dataset directory\n",
    "val_file = os.path.join(DATA_DIR, 'val.json')\n",
    "#x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "#y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
    "\n",
    "# Testing dataset directory\n",
    "test_file = os.path.join(DATA_DIR, 'test.json')\n",
    "#x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "#y_test_dir = os.path.join(DATA_DIR, 'testannot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8b4e8-7c12-4d49-a6da-451df2206415",
   "metadata": {},
   "source": [
    "### 3.2. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a585316-389e-4254-b441-153db080844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e288b36-1c4a-445a-8907-59df86a11612",
   "metadata": {},
   "source": [
    "### 3.3. Dataloader\n",
    "Helper class for data extraction, transformation and preprocessing.\n",
    "\n",
    "At the moment, we do not have one label per image (we have less labels than images, given our preprocessing methods), so we will extract image id from the annotations and will thus not be using all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04043c9d-1631-4375-bd2f-f479ca53c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"Solar Panel dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['solar panel']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            data_dir,\n",
    "            #images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        \n",
    "        # Opening Json file with data and storing it in array\n",
    "        with open(data_dir) as json_file:\n",
    "            data_file = json.load(json_file)\n",
    "        \n",
    "        # Getting images directory from dataset\n",
    "        img_dir = data_file['images'][0]['file_name']\n",
    "        \n",
    "        # Recovering image ids\n",
    "        ids = []\n",
    "        imgs_dir_id = []\n",
    "        masks_dir_id = []\n",
    "        for annot_data in data_file['annotations']:\n",
    "            # Get id\n",
    "            img_id = annot_data['image_id']\n",
    "            ids.append(img_id)\n",
    "            \n",
    "            # Setup paths until image + mask\n",
    "            img_name = str(img_id) + '.tif'\n",
    "            img_filename = os.path.join(IMG_DIR, img_name)\n",
    "            \n",
    "            mask_name = str(img_id) + '.png'\n",
    "            mask_filename = os.path.join(MASKS_DIR, mask_name)\n",
    "            \n",
    "            # Adding it to global arrays\n",
    "            imgs_dir_id.append((img_filename, img_id))\n",
    "            masks_dir_id.append((mask_filanem, img_id))\n",
    "            \n",
    "        \n",
    "        self.ids = ids\n",
    "        self.images_fps = imgs_dir_id\n",
    "        self.masks_fps = masks_dir_id\n",
    "        \n",
    "        \n",
    "        #self.ids = os.listdir(images_dir)\n",
    "        #self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        #self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff7580-2bf8-4ef8-bd0a-4aa771219c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at data we have\n",
    "\n",
    "dataset = Dataset(x_train_dir, y_train_dir, classes=['car'])\n",
    "\n",
    "image, mask = dataset[4] # get some sample\n",
    "visualize(\n",
    "    image=image, \n",
    "    cars_mask=mask.squeeze(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
